{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes-for-Classification\" data-toc-modified-id=\"Naive-Bayes-for-Classification-1\">Naive Bayes for Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#By-The-End-Of-This-Session-You-Should-Be-Able-To:\" data-toc-modified-id=\"By-The-End-Of-This-Session-You-Should-Be-Able-To:-1.1\">By The End Of This Session You Should Be Able To:</a></span></li><li><span><a href=\"#What-is-classification?\" data-toc-modified-id=\"What-is-classification?-1.2\">What is classification?</a></span></li><li><span><a href=\"#Methods-for-Classification\" data-toc-modified-id=\"Methods-for-Classification-1.3\">Methods for Classification</a></span></li><li><span><a href=\"#Rules-for-Text-Classification\" data-toc-modified-id=\"Rules-for-Text-Classification-1.4\">Rules for Text Classification</a></span></li><li><span><a href=\"#What-are-the-downsides-of-rules?\" data-toc-modified-id=\"What-are-the-downsides-of-rules?-1.5\">What are the downsides of rules?</a></span></li><li><span><a href=\"#Statistical-Learning,-aka-Machine-Learning\" data-toc-modified-id=\"Statistical-Learning,-aka-Machine-Learning-1.6\">Statistical Learning, aka Machine Learning</a></span></li><li><span><a href=\"#Machine-Learning-for-Classification\" data-toc-modified-id=\"Machine-Learning-for-Classification-1.7\">Machine Learning for Classification</a></span></li><li><span><a href=\"#Machine-Learning->-Static-Rules\" data-toc-modified-id=\"Machine-Learning->-Static-Rules-1.8\">Machine Learning &gt; Static Rules</a></span></li><li><span><a href=\"#Different-Models-of-The-World\" data-toc-modified-id=\"Different-Models-of-The-World-1.9\">Different Models of The World</a></span></li><li><span><a href=\"#What-is-Bayes-Rule?\" data-toc-modified-id=\"What-is-Bayes-Rule?-1.10\">What is Bayes Rule?</a></span></li><li><span><a href=\"#Why-do-we-want-to-use-Bayes-Rule-as-our-ML-algorithm?\" data-toc-modified-id=\"Why-do-we-want-to-use-Bayes-Rule-as-our-ML-algorithm?-1.11\">Why do we want to use Bayes Rule as our ML algorithm?</a></span></li><li><span><a href=\"#Bayes-Theorem-for-Classification\" data-toc-modified-id=\"Bayes-Theorem-for-Classification-1.12\">Bayes Theorem for Classification</a></span></li><li><span><a href=\"#Likelihood\" data-toc-modified-id=\"Likelihood-1.13\">Likelihood</a></span></li><li><span><a href=\"#Class-Prior\" data-toc-modified-id=\"Class-Prior-1.14\">Class Prior</a></span></li><li><span><a href=\"#Predictor-Prior-Probability\" data-toc-modified-id=\"Predictor-Prior-Probability-1.15\">Predictor Prior Probability</a></span></li><li><span><a href=\"#Posterior-Probability\" data-toc-modified-id=\"Posterior-Probability-1.16\">Posterior Probability</a></span></li><li><span><a href=\"#Naive-Bayes-for-Text-Classification\" data-toc-modified-id=\"Naive-Bayes-for-Text-Classification-1.17\">Naive Bayes for Text Classification</a></span></li><li><span><a href=\"#Dropping-p(doc)-from-the-equation\" data-toc-modified-id=\"Dropping-p(doc)-from-the-equation-1.18\">Dropping p(doc) from the equation</a></span></li><li><span><a href=\"#Prediction-the-class-for-a-new-document\" data-toc-modified-id=\"Prediction-the-class-for-a-new-document-1.19\">Prediction the class for a new document</a></span></li><li><span><a href=\"#Being-Naive:-Bayes'-Greatest-Strength-&amp;-Greatest-Weakness\" data-toc-modified-id=\"Being-Naive:-Bayes'-Greatest-Strength-&amp;-Greatest-Weakness-1.20\">Being Naive: Bayes' Greatest Strength &amp; Greatest Weakness</a></span></li><li><span><a href=\"#Naive:-Each-word-conditionally-independent-for-each-label\" data-toc-modified-id=\"Naive:-Each-word-conditionally-independent-for-each-label-1.21\">Naive: Each word conditionally independent for each label</a></span></li><li><span><a href=\"#Naive-Bayes-works-very-well-in-practice\" data-toc-modified-id=\"Naive-Bayes-works-very-well-in-practice-1.22\">Naive Bayes works very well in practice</a></span></li><li><span><a href=\"#Naive-Bayes-works-very-well-in-practice\" data-toc-modified-id=\"Naive-Bayes-works-very-well-in-practice-1.23\">Naive Bayes works very well in practice</a></span></li><li><span><a href=\"#In-text-classification-features-are-critical\" data-toc-modified-id=\"In-text-classification-features-are-critical-1.24\">In text classification features are critical</a></span></li><li><span><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-1.25\">Discussion</a></span></li><li><span><a href=\"#What-about-Deep-Learning-for-Classification?\" data-toc-modified-id=\"What-about-Deep-Learning-for-Classification?-1.26\">What about Deep Learning for Classification?</a></span></li><li><span><a href=\"#Peter-Norvig-on-Naive-Bayes-at-Google\" data-toc-modified-id=\"Peter-Norvig-on-Naive-Bayes-at-Google-1.27\">Peter Norvig on Naive Bayes at Google</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-1.28\">Check for understanding</a></span></li><li><span><a href=\"#Any-questions-about-discrete-Naive-Bayes-before-moving-on\" data-toc-modified-id=\"Any-questions-about-discrete-Naive-Bayes-before-moving-on-1.29\">Any questions about discrete Naive Bayes before moving on</a></span></li><li><span><a href=\"#Gaussian-Naive-Bayes:-1-dimension\" data-toc-modified-id=\"Gaussian-Naive-Bayes:-1-dimension-1.30\">Gaussian Naive Bayes: 1 dimension</a></span></li><li><span><a href=\"#Gaussian-Naive-Bayes:-2-dimensions\" data-toc-modified-id=\"Gaussian-Naive-Bayes:-2-dimensions-1.31\">Gaussian Naive Bayes: 2 dimensions</a></span></li><li><span><a href=\"#Gaussian-Naive-Bayes:-2-dimensions\" data-toc-modified-id=\"Gaussian-Naive-Bayes:-2-dimensions-1.32\">Gaussian Naive Bayes: 2 dimensions</a></span></li><li><span><a href=\"#When-does-Naive-Bayes-NOT-work,-aka-make-useful-predictions?\" data-toc-modified-id=\"When-does-Naive-Bayes-NOT-work,-aka-make-useful-predictions?-1.33\">When does Naive Bayes NOT work, aka make useful predictions?</a></span></li><li><span><a href=\"#Further-Study\" data-toc-modified-id=\"Further-Study-1.34\">Further Study</a></span></li><li><span><a href=\"#Summary,-Part-I\" data-toc-modified-id=\"Summary,-Part-I-1.35\">Summary, Part I</a></span></li><li><span><a href=\"#Summary,-Part-II\" data-toc-modified-id=\"Summary,-Part-II-1.36\">Summary, Part II</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-1.37\">Bonus Material</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Naive Bayes for Classification</h1></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain each element of the Bayes Theorem \n",
    "- Apply Bayes Theorem to classification\n",
    "- Explain when and when __not__ to apply Naive Bayes\n",
    "- Walk through a example of Naive Bayes text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is classification?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predict a single category from a collection of discrete categories.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Methods for Classification\n",
    "-----\n",
    "\n",
    "1. Hand-written Rules\n",
    "1. Statistical Learning\n",
    "1. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rules for Text Classification\n",
    "----\n",
    "\n",
    "Write a series of `if then` statements by hand\n",
    "\n",
    "Examples:\n",
    "\n",
    "- If \"Viagra\" or \"Nigerian Prince\" appear in an email, then email is spam.\n",
    "- If words such as \"worst ever\" and \"terrible\" appear in a movie review, then the review is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the downsides of rules?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Time intensive - To be complete, a large set of rules would be needed.\n",
    "1. Brittle - Rules could not easily apply to new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Statistical Learning, aka Machine Learning\n",
    "-----\n",
    "\n",
    "Learn from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine Learning for Classification\n",
    "------\n",
    "<center><img src=\"images/pipeline.jpg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Bayes Rule</h2></center>\n",
    "<center><img src=\"images/Thomas_Bayes.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*l0MccMHzSjtpJ_mGaItVfA.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do we want to use Bayes Rule as our ML algorithm?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://static1.squarespace.com/static/591e58f72994cab66b93f891/t/591f89a39c03e001d1e3b17e/1495241602853/bayes-rule-e1350930203949.png\" width=\"85%\"/></center>\n",
    "\n",
    "Allows us to evaluate our hypothesis, given our prior beliefs and new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayes Theorem for Classification\n",
    "-----\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Likelihood\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "How likely we are to observe the data point given a specific label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Class Prior\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "How likely we are to observe each of the label, ignoring the data.\n",
    "\n",
    "Also called base-rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Predictor Prior Probability\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "How likely we are to observe any given data point $p(doc)$ \n",
    "\n",
    "In the case of text classification, probability of observing an individual document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Posterior Probability\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "Predict probability of a label given a new observation.\n",
    "\n",
    "The goal of machine learning classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes for Text Classification\n",
    "------\n",
    "\n",
    "  $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    "  \n",
    "\n",
    "- $p(\\text{class }| \\text{ doc})$: observing a particular class given a document (Posterior)\n",
    "- $p(\\text{doc } | \\text{ class})$: observing a document given a particular class (Likelihood)\n",
    "- $p(\\text{class})$: observing each of the classes (Prior)\n",
    "- $p(\\text{doc})$: observing an individual document (Marginal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dropping p(doc) from the equation\n",
    "-----\n",
    "\n",
    "Since all calculated probabilities have $p(\\text{doc})$ as their denominator, we can eliminate the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prediction the class for a new document\n",
    "-------\n",
    "\n",
    "$ P(\\text{class } |\\text{ doc}) = P(\\text{class }) •  \\prod_{i=1}^n P(word_i | \\text{class })$\n",
    "\n",
    "$... = P(\\text{class}) \\times (P(word_1 \\text{ | class}) \\cdot P(word_2 \\text{ | class}) \\cdot ... \\cdot P(word_n \\text{ | class}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Being Naive: Bayes' Greatest Strength & Greatest Weakness\n",
    "------\n",
    "\n",
    "<center><img src=\"images/naive.jpeg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive: Each word conditionally independent for each label\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "Each word can independently predict multiple labels.\n",
    "\n",
    "For the word \"Viagra\", there could be a high probability of `spam` but could also have a high probability of `ham` (not spam). Let's say you worked as a pharmaceutical salesperson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes works very well in practice\n",
    "----\n",
    "\n",
    "__Despite feature independence assumption__, NB is very common and useful in real world applications: Email filtering, Fraud detection, Medical diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In text classification features are critical\n",
    "-------\n",
    "<center><img src=\"images/supervised_learning.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Discussion\n",
    "------\n",
    "\n",
    "What features might you want to use for text classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Data:\n",
    "    - Individual words\n",
    "    - Phrases\n",
    "    - Punctuation\n",
    "- Metadata:\n",
    "    - Length of document\n",
    "    - Language\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What about Deep Learning for Classification?\n",
    "-----\n",
    "\n",
    "Deep Learning is state-of-the-art (STOTA) because it can automatically learn features and the classifier.\n",
    "\n",
    "But sometimes, you don't have enough data or time.\n",
    "\n",
    "Naive Bayes is fast and works with very few examples. It is \"good enough\" for many uses cases and a very powerful baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Peter Norvig on Naive Bayes at Google\n",
    "-----\n",
    "\n",
    "> And it was fun looking at the comments, because you’d see things like ‘well, I’m throwing in this naive Bayes now, but I’m gonna come back and fix it it up and come up with something better later. And the comment would be from several years ago\n",
    "\n",
    "> … when you have enough data, sometimes, you don’t have to be too clever about coming up with the best algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Why is Naive Bayes \"embarrassingly parallelizable\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each feature weight can be learned separately for each class.\n",
    "\n",
    "Thus can be learned at the same time on a separate computer core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://alitarhini.wordpress.com/2011/03/02/parallel-naive-bayesian-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Any questions about discrete Naive Bayes before moving on</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Naive Bayes: 1 dimension\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Yune_Lee/publication/255695722/figure/fig1/AS:297967207632900@1448052327024/Illustration-of-how-a-Gaussian-Naive-Bayes-GNB-classifier-works-For-each-data-point.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Naive Bayes: 2 dimensions\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/two_d.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Naive Bayes: 2 dimensions\n",
    "-----\n",
    "<center><img src=\"images/two_d_bound.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Assumes sigma hyperparamter is shard across classes.\n",
    "\n",
    "The boundary can be nonlinear is sigma hyperparameter is different for different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When does Naive Bayes NOT work, aka make useful predictions?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/anti_bayes.png\" width=\"67%\"/></center>\n",
    "\n",
    "These features are __not__ independent.\n",
    "\n",
    "Modeling correlation between features is needed to separate classes.\n",
    "\n",
    "These groups are separable but a NB classifier will perform at chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.youtube.com/watch?v=feBKiAdhYkc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Bayes Rule is one of the most important theorems in Statistics.\n",
    "- You should be apply to write Bayes Rule from memory and able to Bayes Rule to novel situations.\n",
    "- Naive Bayes (NB) is among the simplest (and most effective) machine learning classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary, Part II\n",
    "------\n",
    " \n",
    "- Discrete NB works well for text classification.\n",
    "- However, NB does not work when features correlation are needed to separate classes.\n",
    "- Gaussian Naive Bayes is a logical extension for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes works very well in practice\n",
    "----\n",
    "\n",
    "<center><img src=\"http://www.azquotes.com/picture-quotes/quote-well-it-may-be-all-right-in-practice-but-it-will-never-work-in-theory-warren-buffett-86-75-88.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "__Despite feature independence assumption__, NB is very common in real world applications: Email filtering, Fraud detection, Medical diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------\n",
    "\n",
    "- [Guassian Naive Bayes formula](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes)\n",
    "- Modeling a combination of discrete and real-valued features\n",
    "- Non-guassian Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
