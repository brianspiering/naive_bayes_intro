{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Implement Naive Bayes From Scratch</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes' Theorem</h2></center>\n",
    "\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Steps for Training Naive Bayes</h2></center>\n",
    "\n",
    "1. Acquire labeled data\n",
    "1. Preprocess data\n",
    "1. Calculate document class priors\n",
    "1. Calculate word by class conditional probabilities\n",
    "1. Calculate the proportional probabilities for each class of new document\n",
    "1. Pick the winning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Acquire data & preprocess\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus  = [\"🐈 🐯 🐱 🐩 🐱\", \n",
    "           \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\", \n",
    "           \"🐈 🐈 🐯 🐶 🐈\",  \n",
    "           \"🐈 🐈 🐈\",\n",
    "           \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \"]\n",
    "\n",
    "labels = ['cat', 'dog', 'cat', 'cat','dog'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = zip(labels, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 🐈 🐯 🐱 🐩 🐱\n",
      "dog: 🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\n",
      "cat: 🐈 🐈 🐯 🐶 🐈\n",
      "cat: 🐈 🐈 🐈\n",
      "dog: 🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \n"
     ]
    }
   ],
   "source": [
    "for label, item in data:\n",
    "    print(f\"{label}: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Calculate document class priors</h2></center>\n",
    "\n",
    "$$P(c) = \\frac{N_c}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat', 'dog'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What labels are we dealing with?\n",
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many documents are dealing with?\n",
    "n_docs = len(corpus)\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revise here\n",
    "\n",
    "add something like this\n",
    "\n",
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum()   # spam_messages already defined in a cell above\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    \n",
    "    n_word_given_ham = ham_messages[word].sum()   # ham_messages already defined in a cell above\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 0.6)\n",
      "('dog', 0.4)\n"
     ]
    }
   ],
   "source": [
    "# For each label, find the probability of baseline occurance\n",
    "doc_priors = defaultdict(float)\n",
    "\n",
    "for label in labels:\n",
    "    doc_priors[label] = sum(1 for d in train if d.label == label) / n_docs\n",
    "\n",
    "print(*doc_priors.items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate conditional probabilities of each word for each class\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['🐈', '🐯', '🐱', '🐩', '🐱', '🐶', '🐶', '🐈', '🐶', '🐩', '🐈', '🐶', '🐶', '🐈', '🐈', '🐯', '🐶', '🐈', '🐈', '🐈', '🐈', '🐶', '🐶', '🐯', '🐈', '🐩', '🐱', '🐩', '🐶', '🐩', '🐶']\n"
     ]
    }
   ],
   "source": [
    "# Get all tokens, aka the vocabulary\n",
    "vocab = []\n",
    "\n",
    "for doc in train:\n",
    "    vocab.extend(doc.tokens)\n",
    "    \n",
    "print(\"Vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'🐈', '🐩', '🐯', '🐱', '🐶'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique tokens\n",
    "set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality of vocab: 5\n"
     ]
    }
   ],
   "source": [
    "# Number of unique tokens, aka cardinality\n",
    "v = len(set(vocab))\n",
    "print(\"Cardinality of vocab:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'cat': defaultdict(float,\n",
       "                         {'🐈': 0.5384615384615384,\n",
       "                          '🐯': 0.15384615384615385,\n",
       "                          '🐱': 0.15384615384615385,\n",
       "                          '🐩': 0.07692307692307693,\n",
       "                          '🐶': 0.07692307692307693}),\n",
       "             'dog': defaultdict(float,\n",
       "                         {'🐈': 0.16666666666666666,\n",
       "                          '🐯': 0.05555555555555555,\n",
       "                          '🐱': 0.05555555555555555,\n",
       "                          '🐩': 0.2222222222222222,\n",
       "                          '🐶': 0.5})})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A default dict of default dicts; inner default dict is probability\n",
    "cond_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for label in labels:\n",
    "    \n",
    "    label_tokens = []\n",
    "    for doc in train:\n",
    "         # For a given label, get a list of all the tokens for all the docs \n",
    "        if doc.label == label:\n",
    "            label_tokens.extend(doc.tokens)\n",
    "\n",
    "    for token in vocab:\n",
    "        # Find conditional probability: token count / total count\n",
    "        cond_prob[label][token] = label_tokens.count(token) / len(label_tokens) \n",
    "\n",
    "cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that each label is a probability mass function (pmf). A pmf sums to 1\n",
    "from math import isclose\n",
    "\n",
    "for label in labels:\n",
    "    assert isclose(sum(cond_prob[label].values()), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Steps for Predicting with Naive Bayes</h2></center>\n",
    "\n",
    "1. Acquire and process data\n",
    "1. Calculate the proportional probabilities for each class of new document\n",
    "1. Pick the winning class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a new document without a label,  calculate the proportional probabilities for each class\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(c | X) = P(c) •  \\prod_{i=1}^n P(x_i | c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "def product(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 0.09230769230769231)\n",
      "('dog', 0.022222222222222223)\n"
     ]
    }
   ],
   "source": [
    "test = LabeledTextData(id_num=90, label=None, tokens=\"🐱\".split())\n",
    "# test = LabeledTextData(id_num=91, label=None, tokens=\"🐶 🐶\".split()) \n",
    "# test = LabeledTextData(id_num=92, label=None, tokens=\"🐶 🐱\".split())\n",
    "# test = LabeledTextData(id_num=93, label=None, tokens=\"🐈 🐈 🐶 🐶 🐩 🐱 🐱\".split())\n",
    "# test = LabeledTextData(id_num=94, label=None, tokens=\"🐬 \".split()) # Out of sample prediction\n",
    "\n",
    "prob_predicted = defaultdict(float)\n",
    "for label in labels:\n",
    "    # For each label, calculate the conditional probability based on the prior and the tokens that appear\n",
    "    prob_predicted[label] = doc_priors[label] * product(cond_prob[label][t] for t in test.tokens)\n",
    "    \n",
    "print(*dict(prob_predicted).items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pick the winning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: cat\n"
     ]
    }
   ],
   "source": [
    "label, prob = max(prob_predicted.items(), key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Handle ties and fall back to document priors if winning probability is zero\n",
    "\n",
    "label, prob = max(prob_predicted.items(), key=itemgetter(1))\n",
    "if prob > 0:\n",
    "    print(\"The predicted class is: \", end=\"\")\n",
    "    print(*(k for k, v in prob_predicted.items() if v == prob))\n",
    "else:\n",
    "    label, prob = max(doc_priors.items(),\n",
    "                      key=itemgetter(1))\n",
    "    print(\"The predicted class is:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Other implementations by Brian Spiering\n",
    "    - [Using NormalDist from statistics module](https://github.com/brianspiering/naive_bayes_classifer_in_python_3_8)\n",
    "    - [Naive Bayes for Text Classification](https://github.com/brianspiering/bayesian-text)\n",
    "    \n",
    "- [Implementation by mircealex](https://github.com/mircealex)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
