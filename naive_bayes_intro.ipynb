{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Naive Bayes Introduction</h1></center>\n",
    "\n",
    "<center><img src=\"images/dare_to_be_naive.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Who am I?</h2></center>\n",
    "<center><img src=\"images/hi.png\" width=\"37%\"/></center>\n",
    "\n",
    "<center>Brian Spiering</center>\n",
    "\n",
    "<center>Data Science Instructor</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Naive Bayes Overview</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Agenda</h2></center>\n",
    "\n",
    "1. Intro to machine learning\n",
    "1. Naive Bayes overview \n",
    "1. Naive Bayes from scratch\n",
    "1. Naive Bayes in scikit-learn\n",
    "1. Questions & Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is classification?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Predicting a single category from a collection of discrete categories. </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Dog or Lion?</h2></center>\n",
    "<center><img src=\"images/prediction.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification Methods</h2></center>\n",
    "\n",
    "1. Hand-written Rules\n",
    "1. Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Hand-written Rules</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Write a series of \"if this then that\" statements by hand.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Hand-written Rules Examples</h2></center>\n",
    "\n",
    "- If \"Viagra\" or \"Nigerian Prince\" appear, then an email is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If words such as \"worst ever\" and \"terrible\" appear in a movie review, then the review is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the downsides of hand written rules?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Time intensive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Brittle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "1. Time intensive \n",
    "    - Require Humans\n",
    "    - To be complete, a large set of rules would be needed.\n",
    "\n",
    "2. Brittle\n",
    "    - Rules could not easily apply to new situations. Either new domains or as domains change over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Machine Learning</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Automatically learn patterns from the data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>If certain words co-occur with certain categories, let's find those relationship by writing computer code.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule</h2></center>\n",
    "<center><img src=\"images/Thomas_Bayes.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule</h2></center>\n",
    "<center><img src=\"images/bayes_with_text.jpeg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TODO: Put in example of using bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule as an ML algorithm</h2></center>\n",
    "\n",
    "<center><img src=\"images/bayes-rule-color.png\" width=\"75%\"/></center>\n",
    "\n",
    "<center>Evaluate our hypothesis, given our prior beliefs and new empirical evidence</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes for Text Classification</h2></center>\n",
    "\n",
    " $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$p(\\text{class }| \\text{ doc}) = \\frac{\\boxed{p(\\text{doc } | \\text{ class})} \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    " \n",
    "$p(\\text{doc } | \\text{ class})$: observing a document given a particular class (Likelihood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times \\boxed{p(\\text{class})}}{p(\\text{doc})}$$\n",
    " \n",
    "$p(\\text{class})$: observing each of the classes (Prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{\\boxed{p(\\text{doc})}}$$\n",
    " \n",
    "$p(\\text{doc})$: observing an individual document (Marginal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>p(doc) is typically dropped</h2></center>\n",
    "\n",
    " $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    " \n",
    "Since <b>all</b> calculated probabilities have $p(\\text{doc})$ as their denominator, we can drop $p(\\text{doc})$ from the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " $$\\boxed{p(\\text{class }| \\text{ doc})} = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    " \n",
    "$p(\\text{class }| \\text{ doc})$: observing a particular class given a document (Posterior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>New document prediction </h2></center>\n",
    "\n",
    "$$P(\\text{class } |\\text{ doc}) =   (P(word_1 \\text{ | class}) \\newline \\times P(word_2 \\text{ | class}) ... \\times P(word_n \\text{ | class})) \\times P(\\text{class})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>New document prediction </h2></center>\n",
    "\n",
    "$$ P(\\text{class } |\\text{ doc}) = \\prod_{i=1}^n P(word_i | \\text{class })\\newline \\times  P(\\text{class })$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Being Naive: <br> Bayes' Greatest Strength & Greatest Weakness</h2></center>\n",
    "<center><img src=\"images/dare_to_be_naive.jpeg\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive: Each word is conditionally independent for each label\n",
    "</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Naive Bayes ignores word order, aka Bag-of-Words assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center>What about combinations of words? </center>\n",
    "<br>\n",
    "<center>For example, \"not terrible\" in a movie review</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes works in practice</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/buffet.jpeg\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Despite feature independence assumption, Naive Bayes is very common and useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Real-world applications</h2></center>\n",
    "\n",
    "- Fraud detection 🕵️‍♀️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Is a review positive 👍 or negative 👎?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Medical diagnosis (➕ or ➖)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Implement Naive Bayes <br>From Scratch</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes' Theorem</h2></center>\n",
    "\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Training Naive Bayes</h2></center>\n",
    "\n",
    "1. Acquire labeled data\n",
    "1. Preprocess data\n",
    "1. Calculate document class priors\n",
    "1. Calculate word by class conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Acquire data & preprocess\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus  = [\"🐈 🐯 🐱 🐩 🐱\", \n",
    "           \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\", \n",
    "           \"🐈 🐈 🐯 🐶 🐈\",  \n",
    "           \"🐈 🐈 🐈\",\n",
    "           \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \"]\n",
    "\n",
    "labels = ['cat', 'dog', 'cat', 'cat','dog'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "data = [['cat', \"🐈 🐯 🐱 🐩 🐱\"],\n",
    "        ['dog', \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\"],\n",
    "        ['cat', \"🐈 🐈 🐯 🐶 🐈\"],\n",
    "        ['cat', \"🐈 🐈 🐈\"], \n",
    "        ['dog', \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 🐈 🐯 🐱 🐩 🐱\n",
      "dog: 🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\n",
      "cat: 🐈 🐈 🐯 🐶 🐈\n",
      "cat: 🐈 🐈 🐈\n",
      "dog: 🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \n"
     ]
    }
   ],
   "source": [
    "for label, item in data:\n",
    "    print(f\"{label}: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Calculate class priors</h2></center>\n",
    "\n",
    "$$P(c) = \\frac{N_c}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat', 'dog'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What labels are we dealing with?\n",
    "labels = set(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many documents are dealing with?\n",
    "n_docs = len(corpus)\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "doc_priors = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dog', 0.4)\n",
      "('cat', 0.6)\n"
     ]
    }
   ],
   "source": [
    "# For each label, find the probability of baseline occurance\n",
    "for label in labels:\n",
    "    doc_priors[label] = sum(1 for item_label, _ in data if item_label == label) / n_docs\n",
    "\n",
    "print(*doc_priors.items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Conditional probabilities of word by class</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['🐈', '🐯', '🐱', '🐩', '🐱', '🐶', '🐶', '🐈', '🐶', '🐩', '🐈', '🐶', '🐶', '🐈', '🐈', '🐯', '🐶', '🐈', '🐈', '🐈', '🐈', '🐶', '🐶', '🐯', '🐈', '🐩', '🐱', '🐩', '🐶', '🐩', '🐶']\n"
     ]
    }
   ],
   "source": [
    "# Get all tokens, aka the vocabulary\n",
    "for _, doc in data:\n",
    "    vocab.extend(doc.split())\n",
    "    \n",
    "print(\"Vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'🐈', '🐩', '🐯', '🐱', '🐶'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique tokens\n",
    "set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality of vocab: 5\n"
     ]
    }
   ],
   "source": [
    "# Number of unique tokens, aka cardinality\n",
    "v = len(set(vocab))\n",
    "print(\"Cardinality of vocab:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# A default dict of default dicts; inner default dict is probability\n",
    "cond_prob = defaultdict(lambda: defaultdict(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for label in labels:    \n",
    "    label_tokens = []\n",
    "    for item_label, doc in data:\n",
    "        # For a given label, get a list of all the tokens for all the docs \n",
    "        if item_label == label:\n",
    "            label_tokens.extend(doc.split())\n",
    "\n",
    "    for token in vocab:\n",
    "        # Find conditional probability: token count / total count\n",
    "        cond_prob[label][token] = label_tokens.count(token) / len(label_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog :\n",
      "🐈 0.16666666666666666\n",
      "🐯 0.05555555555555555\n",
      "🐱 0.05555555555555555\n",
      "🐩 0.2222222222222222\n",
      "🐶 0.5\n",
      "Cat :\n",
      "🐈 0.5384615384615384\n",
      "🐯 0.15384615384615385\n",
      "🐱 0.15384615384615385\n",
      "🐩 0.07692307692307693\n",
      "🐶 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "for label, sub_dict in cond_prob.items():\n",
    "    print(label.title(), \":\")\n",
    "    for token, prob in sub_dict.items():\n",
    "        print(token, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Test that each label is a probability mass function (pmf). A pmf sums to 1\n",
    "from math import isclose\n",
    "\n",
    "for label in labels:\n",
    "    assert isclose(sum(cond_prob[label].values()), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Predicting with Naive Bayes</h2></center>\n",
    "\n",
    "1. Acquire and process the new data\n",
    "1. For each new data point, calculate the proportional probabilities for each class\n",
    "1. Pick the winning class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Given a new document, <br> calculate the proportional probabilities for each class</h2></center>\n",
    "\n",
    "$$ P(c | X) = P(c) •  \\prod_{i=1}^n P(x_i | c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Define product function\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "def product(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dog', 0.0010973936899862826)\n",
      "('cat', 0.0005461993627674101)\n"
     ]
    }
   ],
   "source": [
    "new_input = \"🐱\"\n",
    "# new_input = \"🐱 🐩 \"\n",
    "# new_input = \"🐱 🐩 🐩\"\n",
    "\n",
    "prob_predicted = defaultdict(float)\n",
    "for label in labels:\n",
    "    # For each label, calculate the conditional probability based on the prior and the tokens that appear\n",
    "    prob_predicted[label] = doc_priors[label] * product(cond_prob[label][t] for t in new_input.split())\n",
    "    \n",
    "print(*dict(prob_predicted).items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pick the winning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: dog\n"
     ]
    }
   ],
   "source": [
    "label, prob = max(prob_predicted.items(), key=itemgetter(1))\n",
    "print(\"The predicted class is: \", end=\"\")\n",
    "print(*(k for k, v in prob_predicted.items() if v == prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: dog\n"
     ]
    }
   ],
   "source": [
    "# Handle ties and fall back to document priors if winning probability is zero\n",
    "\n",
    "label, prob = max(prob_predicted.items(), key=itemgetter(1))\n",
    "if prob > 0:\n",
    "    print(\"The predicted class is: \", end=\"\")\n",
    "    print(*(k for k, v in prob_predicted.items() if v == prob))\n",
    "else:\n",
    "    label, prob = max(doc_priors.items(),\n",
    "                      key=itemgetter(1))\n",
    "    print(\"The predicted class is:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>\n",
    "\n",
    "- Other implementations by Brian Spiering\n",
    "    - [Using NormalDist from statistics module](https://github.com/brianspiering/naive_bayes_classifer_in_python_3_8)\n",
    "    - [Naive Bayes for Text Classification](https://github.com/brianspiering/bayesian-text)\n",
    "    \n",
    "- [Implementation by mircealex](https://github.com/mircealex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97aaae1a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a13b667",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a0296",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Naive Bayes in Scikit-learn</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "690dae07",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "corpus  = [\"🐈 🐯 🐱 🐩 🐱\", \n",
    "           \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\", \n",
    "           \"🐈 🐈 🐯 🐶 🐈\",  \n",
    "           \"🐈 🐈 🐈\",\n",
    "           \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \"]\n",
    "\n",
    "y = ['cat', 'dog', 'cat', 'cat','dog'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1751679",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 🐈 🐯 🐱 🐩 🐱\n",
      "dog: 🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\n",
      "cat: 🐈 🐈 🐯 🐶 🐈\n",
      "cat: 🐈 🐈 🐈\n",
      "dog: 🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \n"
     ]
    }
   ],
   "source": [
    "for target, item,  in zip(y, corpus):\n",
    "    print(f\"{target}: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "614e6925",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'[^\\s]+')\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12d05c8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🐈' '🐩' '🐯' '🐱' '🐶']\n",
      "[[1 1 1 2 0]\n",
      " [2 1 0 0 5]\n",
      " [3 0 1 0 1]\n",
      " [3 0 0 0 0]\n",
      " [1 3 1 1 4]]\n"
     ]
    }
   ],
   "source": [
    "# Introspect features\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6f9cb80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00d15a04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat']\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data: \"🐈 🐯 🐱 🐩 🐱\"\n",
    "print(clf.predict([[1, 1, 1, 2, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "565266b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat']\n"
     ]
    }
   ],
   "source": [
    "# Prediction on unseen data\n",
    "new_input = \"🐱\"\n",
    "# new_input = \"🐱 🐩 \"\n",
    "# new_input = \"🐱 🐩 🐩\"\n",
    "vectorized = vectorizer.transform([new_input])\n",
    "print(clf.predict(vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b58b2a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features - Probability\n",
      "\n",
      "'cat' category:\n",
      "🐈 - 0.444\n",
      "🐩 - 0.111\n",
      "🐯 - 0.167\n",
      "🐱 - 0.167\n",
      "🐶 - 0.111\n"
     ]
    }
   ],
   "source": [
    "# Let's interpert the feature weights\n",
    "print(\"Features - Probability\", end=\"\\n\\n\")\n",
    "print(\"'cat' category:\")\n",
    "for feature, log_prob_ in zip(vectorizer.get_feature_names_out(), clf.feature_log_prob_[0]):\n",
    "    print(f\"{feature} - {np.exp(log_prob_):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af091777",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features - Probability\n",
      "\n",
      "'dog' category:\n",
      "🐈 - 0.174\n",
      "🐩 - 0.217\n",
      "🐯 - 0.087\n",
      "🐱 - 0.087\n",
      "🐶 - 0.435\n"
     ]
    }
   ],
   "source": [
    "# Let's interpert the feature weights\n",
    "print(\"Features - Probability\", end=\"\\n\\n\")\n",
    "print(\"'dog' category:\")\n",
    "for feature, log_prob_ in zip(vectorizer.get_feature_names_out(), clf.feature_log_prob_[1]):\n",
    "    print(f\"{feature} - {np.exp(log_prob_):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d29ce2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee653d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- https://stackoverflow.com/questions/57333183/sk-learn-countvectorizer-keeping-emojis-as-words\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes Takeaways</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Learns how features are associated with categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Simply & Effective: a great choice for a baseline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes Takeaways</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Straightforward to implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) However, you should use scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>All These Materials <br> (and bonus material)</h2></center>\n",
    "\n",
    "[bit.ly/naive-bayes-intro](http:bit.ly/naive-bayes-intro)   \n",
    "or   \n",
    "[github.com/brianspiering/naive_bayes_intro](http://github.com/brianspiering/naive_bayes_intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule for Classification</h2></center> \n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Likelihood</h2></center>\n",
    "\n",
    "How likely we are to observe the data point given a specific label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Class Prior Probability</h2></center>\n",
    "\n",
    "How likely we are to observe each of the label, ignoring the data?\n",
    "\n",
    "Also called base-rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Predictor Prior Probability</h2></center>\n",
    "\n",
    "\n",
    "How likely we are to observe any given data point?\n",
    "\n",
    "In the case of text classification, probability of observing an individual document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Posterior Probability</h2></center>\n",
    "\n",
    "\n",
    "Predict probability of a label given a new observation.\n",
    "\n",
    "The goal of machine learning classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>In text classification features are critical</h2></center>\n",
    "\n",
    "<center><img src=\"images/supervised_learning.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>What features might you want to use for text classification? </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Data:\n",
    "    - Individual words\n",
    "    - Phrases\n",
    "    - Punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Metadata:\n",
    "    - Length of document\n",
    "    - Language\n",
    "    - Sender or receiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Naive Bayes \"embarrassingly parallelizable\"</h2></center>\n",
    "\n",
    "Each feature weight can be learned separately for each class.\n",
    "\n",
    "Thus it can be learned at the same time on a separate computer / core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Source: https://alitarhini.wordpress.com/2011/03/02/parallel-naive-bayesian-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
