{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Naive Bayes Introduction</h1></center>\n",
    "\n",
    "<center><img src=\"images/dare_to_be_naive.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Who am I?</h2></center>\n",
    "<center><img src=\"images/hi.png\" width=\"37%\"/></center>\n",
    "\n",
    "<center>Brian Spiering</center>\n",
    "\n",
    "<center>Data Science Instructor</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Agenda</h2></center>\n",
    "\n",
    "1. Intro to machine learning\n",
    "1. Naive Bayes overview \n",
    "1. Naive Bayes from scratch\n",
    "1. Naive Bayes in scikit-learn\n",
    "1. Questions & Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is classification?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Predicting a single category from a collection of discrete categories. </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Dog or Lion?</h2></center>\n",
    "<center><img src=\"images/prediction.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification Methods</h2></center>\n",
    "\n",
    "1. Hand-written Rules\n",
    "1. Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Hand-written Rules</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Write a series of \"if this then that\" statements by hand.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Hand-written Rules Examples</h2></center>\n",
    "\n",
    "- If \"Viagra\" or \"Nigerian Prince\" appear, then an email is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If words such as \"worst ever\" and \"terrible\" appear in a movie review, then the review is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the downsides of hand written rules?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Time intensive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Brittle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "1. Time intensive \n",
    "    - Require Humans\n",
    "    - To be complete, a large set of rules would be needed.\n",
    "\n",
    "2. Brittle\n",
    "    - Rules could not easily apply to new situations. Either new domains or as domains change over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Machine Learning</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Automatically learn patterns from the data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>If certain words co-occur with certain categories, let's find those relationship by writing computer code.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule</h2></center>\n",
    "<center><img src=\"images/Thomas_Bayes.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule</h2></center>\n",
    "<center><img src=\"images/bayes_with_text.jpeg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TODO: Put in example of using bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule as an ML algorithm</h2></center>\n",
    "\n",
    "<center><img src=\"images/bayes-rule-color.png\" width=\"75%\"/></center>\n",
    "\n",
    "<center>Evaluate our hypothesis, given our prior beliefs and new empirical evidence</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes for Text Classification</h2></center>\n",
    "\n",
    " $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$p(\\text{class }| \\text{ doc}) = \\frac{\\boxed{p(\\text{doc } | \\text{ class})} \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    " \n",
    "$p(\\text{doc } | \\text{ class})$: observing a document given a particular class (Likelihood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times \\boxed{p(\\text{class})}}{p(\\text{doc})}$$\n",
    " \n",
    "$p(\\text{class})$: observing each of the classes (Prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{\\boxed{p(\\text{doc})}}$$\n",
    " \n",
    "$p(\\text{doc})$: observing an individual document (Marginal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since <b>all</b> calculated probabilities have the same $p(\\text{doc})$, we can drop $p(\\text{doc})$ from the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " $$\\boxed{p(\\text{class }| \\text{ doc})} = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    " \n",
    "$p(\\text{class }| \\text{ doc})$: observing a particular class given a document (Posterior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>New document prediction </h2></center>\n",
    "\n",
    "$$P(\\text{class } |\\text{ doc}) =   (P(word_1 \\text{ | class}) \\newline \\times P(word_2 \\text{ | class}) ... \\times P(word_n \\text{ | class})) \\times P(\\text{class})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>New document prediction </h2></center>\n",
    "\n",
    "$$ P(\\text{class } |\\text{ doc}) = \\prod_{i=1}^n P(word_i | \\text{class })\\newline \\times  P(\\text{class })$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Being Naive: <br> Bayes' Greatest Strength & Greatest Weakness</h2></center>\n",
    "<center><img src=\"images/dare_to_be_naive.jpeg\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive: Each word is conditionally independent for each label\n",
    "</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Naive Bayes ignores word order, aka Bag-of-Words assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center>What about combinations of words? </center>\n",
    "<br>\n",
    "<center>For example, \"not terrible\" in a movie review</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes works in practice</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/buffet.jpeg\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Despite feature independence assumption, Naive Bayes is very common and useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Real-world applications</h2></center>\n",
    "\n",
    "- Fraud detection 🕵️‍♀️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Is a review positive 👍 or negative 👎?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Medical diagnosis (➕ or ➖)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Implement Naive Bayes <br>From Scratch</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes' Theorem</h2></center>\n",
    "\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Training Naive Bayes</h2></center>\n",
    "\n",
    "1. Acquire labeled data\n",
    "1. Preprocess data\n",
    "1. Calculate document class priors\n",
    "1. Calculate word by class conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Acquire data & preprocess\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus  = [\"🐈 🐯 🐱 🐩 🐱\", \n",
    "           \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\", \n",
    "           \"🐈 🐈 🐯 🐶 🐈\",  \n",
    "           \"🐈 🐈 🐈\",\n",
    "           \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \"]\n",
    "\n",
    "labels = ['cat', 'dog', 'cat', 'cat','dog'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "data = [['cat', \"🐈 🐯 🐱 🐩 🐱\"],\n",
    "        ['dog', \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\"],\n",
    "        ['cat', \"🐈 🐈 🐯 🐶 🐈\"],\n",
    "        ['cat', \"🐈 🐈 🐈\"], \n",
    "        ['dog', \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 🐈 🐯 🐱 🐩 🐱\n",
      "dog: 🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\n",
      "cat: 🐈 🐈 🐯 🐶 🐈\n",
      "cat: 🐈 🐈 🐈\n",
      "dog: 🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \n"
     ]
    }
   ],
   "source": [
    "for label, item in data:\n",
    "    print(f\"{label}: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Calculate class priors</h2></center>\n",
    "\n",
    "$$P(c) = \\frac{N_c}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat', 'dog'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What labels are we dealing with?\n",
    "labels = set(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many documents are dealing with?\n",
    "n_docs = len(corpus)\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "doc_priors = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dog', 0.4)\n",
      "('cat', 0.6)\n"
     ]
    }
   ],
   "source": [
    "# For each label, find the probability of baseline occurance\n",
    "for label in labels:\n",
    "    doc_priors[label] = sum(1 for item_label, _ in data if item_label == label) / n_docs\n",
    "\n",
    "print(*doc_priors.items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Conditional probabilities of word by class</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['🐈', '🐯', '🐱', '🐩', '🐱', '🐶', '🐶', '🐈', '🐶', '🐩', '🐈', '🐶', '🐶', '🐈', '🐈', '🐯', '🐶', '🐈', '🐈', '🐈', '🐈', '🐶', '🐶', '🐯', '🐈', '🐩', '🐱', '🐩', '🐶', '🐩', '🐶']\n"
     ]
    }
   ],
   "source": [
    "# Get all tokens, aka the vocabulary\n",
    "for _, doc in data:\n",
    "    vocab.extend(doc.split())\n",
    "    \n",
    "print(\"Vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'🐈', '🐩', '🐯', '🐱', '🐶'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique tokens\n",
    "set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality of vocab: 5\n"
     ]
    }
   ],
   "source": [
    "# Number of unique tokens, aka cardinality\n",
    "v = len(set(vocab))\n",
    "print(\"Cardinality of vocab:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# A default dict of default dicts; inner default dict is probability\n",
    "cond_prob = defaultdict(lambda: defaultdict(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for label in labels:    \n",
    "    label_tokens = []\n",
    "    for item_label, doc in data:\n",
    "        # For a given label, get a list of all the tokens for all the docs \n",
    "        if item_label == label:\n",
    "            label_tokens.extend(doc.split())\n",
    "\n",
    "    for token in vocab:\n",
    "        # Find conditional probability: token count / total count\n",
    "        cond_prob[label][token] = label_tokens.count(token) / len(label_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog :\n",
      "🐈 0.16666666666666666\n",
      "🐯 0.05555555555555555\n",
      "🐱 0.05555555555555555\n",
      "🐩 0.2222222222222222\n",
      "🐶 0.5\n",
      "Cat :\n",
      "🐈 0.5384615384615384\n",
      "🐯 0.15384615384615385\n",
      "🐱 0.15384615384615385\n",
      "🐩 0.07692307692307693\n",
      "🐶 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "for label, sub_dict in cond_prob.items():\n",
    "    print(label.title(), \":\")\n",
    "    for token, prob in sub_dict.items():\n",
    "        print(token, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Test that each label is a probability mass function (pmf). A pmf sums to 1\n",
    "from math import isclose\n",
    "\n",
    "for label in labels:\n",
    "    assert isclose(sum(cond_prob[label].values()), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Predicting with Naive Bayes</h2></center>\n",
    "\n",
    "1. Acquire and process the new data\n",
    "1. For each new data point, calculate the proportional probabilities for each class\n",
    "1. Pick the winning class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Given a new document, <br> calculate the proportional probabilities for each class</h2></center>\n",
    "\n",
    "$$ P(c | X) = P(c) •  \\prod_{i=1}^n P(x_i | c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Define product function\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "def product(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dog', 0.022222222222222223)\n",
      "('cat', 0.09230769230769231)\n"
     ]
    }
   ],
   "source": [
    "new_input = \"🐱\"\n",
    "# new_input = \"🐱 🐩 \"\n",
    "# new_input = \"🐱 🐩 🐩\"\n",
    "\n",
    "prob_predicted = defaultdict(float)\n",
    "for label in labels:\n",
    "    # For each label, calculate the conditional probability based on the prior and the tokens that appear\n",
    "    prob_predicted[label] = doc_priors[label] * product(cond_prob[label][t] for t in new_input.split())\n",
    "    \n",
    "print(*dict(prob_predicted).items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pick the winning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: cat\n"
     ]
    }
   ],
   "source": [
    "label, prob = max(prob_predicted.items(), key=itemgetter(1))\n",
    "print(\"The predicted class is: \", end=\"\")\n",
    "print(*(k for k, v in prob_predicted.items() if v == prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: cat\n"
     ]
    }
   ],
   "source": [
    "# Handle ties and fall back to document priors if winning probability is zero\n",
    "\n",
    "label, prob = max(prob_predicted.items(), key=itemgetter(1))\n",
    "if prob > 0:\n",
    "    print(\"The predicted class is: \", end=\"\")\n",
    "    print(*(k for k, v in prob_predicted.items() if v == prob))\n",
    "else:\n",
    "    label, prob = max(doc_priors.items(),\n",
    "                      key=itemgetter(1))\n",
    "    print(\"The predicted class is:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>\n",
    "\n",
    "- Other implementations by Brian Spiering\n",
    "    - [Using NormalDist from statistics module](https://github.com/brianspiering/naive_bayes_classifer_in_python_3_8)\n",
    "    - [Naive Bayes for Text Classification](https://github.com/brianspiering/bayesian-text)\n",
    "    \n",
    "- [Implementation by mircealex](https://github.com/mircealex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Naive Bayes in Scikit-learn</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "corpus  = [\"🐈 🐯 🐱 🐩 🐱\", \n",
    "           \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\", \n",
    "           \"🐈 🐈 🐯 🐶 🐈\",  \n",
    "           \"🐈 🐈 🐈\",\n",
    "           \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \"]\n",
    "\n",
    "y = ['cat', 'dog', 'cat', 'cat','dog'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 🐈 🐯 🐱 🐩 🐱\n",
      "dog: 🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\n",
      "cat: 🐈 🐈 🐯 🐶 🐈\n",
      "cat: 🐈 🐈 🐈\n",
      "dog: 🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \n"
     ]
    }
   ],
   "source": [
    "for target, item,  in zip(y, corpus):\n",
    "    print(f\"{target}: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'[^\\s]+')\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🐈' '🐩' '🐯' '🐱' '🐶']\n",
      "[[1 1 1 2 0]\n",
      " [2 1 0 0 5]\n",
      " [3 0 1 0 1]\n",
      " [3 0 0 0 0]\n",
      " [1 3 1 1 4]]\n"
     ]
    }
   ],
   "source": [
    "# Introspect features\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat']\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data: \"🐈 🐯 🐱 🐩 🐱\"\n",
    "print(clf.predict([[1, 1, 1, 2, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat']\n"
     ]
    }
   ],
   "source": [
    "# Prediction on unseen data\n",
    "new_input = \"🐱\"\n",
    "# new_input = \"🐱 🐩 \"\n",
    "# new_input = \"🐱 🐩 🐩\"\n",
    "vectorized = vectorizer.transform([new_input])\n",
    "print(clf.predict(vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features - Probability\n",
      "\n",
      "'cat' category:\n",
      "🐈 - 0.444\n",
      "🐩 - 0.111\n",
      "🐯 - 0.167\n",
      "🐱 - 0.167\n",
      "🐶 - 0.111\n"
     ]
    }
   ],
   "source": [
    "# Let's interpert the feature weights\n",
    "print(\"Features - Probability\", end=\"\\n\\n\")\n",
    "print(\"'cat' category:\")\n",
    "for feature, log_prob_ in zip(vectorizer.get_feature_names_out(), clf.feature_log_prob_[0]):\n",
    "    print(f\"{feature} - {np.exp(log_prob_):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features - Probability\n",
      "\n",
      "'dog' category:\n",
      "🐈 - 0.174\n",
      "🐩 - 0.217\n",
      "🐯 - 0.087\n",
      "🐱 - 0.087\n",
      "🐶 - 0.435\n"
     ]
    }
   ],
   "source": [
    "# Let's interpert the feature weights\n",
    "print(\"Features - Probability\", end=\"\\n\\n\")\n",
    "print(\"'dog' category:\")\n",
    "for feature, log_prob_ in zip(vectorizer.get_feature_names_out(), clf.feature_log_prob_[1]):\n",
    "    print(f\"{feature} - {np.exp(log_prob_):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- https://stackoverflow.com/questions/57333183/sk-learn-countvectorizer-keeping-emojis-as-words\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes Takeaways</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Learns how features are associated with categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Simply & Effective: a great choice for a baseline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Naive Bayes Takeaways</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Straightforward to implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) However, you should use scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>All These Materials <br> (and bonus materials)</h2></center>\n",
    "\n",
    "[bit.ly/naive-bayes-intro](http:bit.ly/naive-bayes-intro)   \n",
    "or   \n",
    "[github.com/brianspiering/naive_bayes_intro](http://github.com/brianspiering/naive_bayes_intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/questions.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayes Rule for Classification</h2></center> \n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Likelihood</h2></center>\n",
    "\n",
    "How likely we are to observe the data point given a specific label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Class Prior Probability</h2></center>\n",
    "\n",
    "How likely we are to observe each of the label, ignoring the data?\n",
    "\n",
    "Also called base-rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Predictor Prior Probability</h2></center>\n",
    "\n",
    "\n",
    "How likely we are to observe any given data point?\n",
    "\n",
    "In the case of text classification, probability of observing an individual document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Posterior Probability</h2></center>\n",
    "\n",
    "\n",
    "Predict probability of a label given a new observation.\n",
    "\n",
    "The goal of machine learning classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>In text classification features are critical</h2></center>\n",
    "\n",
    "<center><img src=\"images/supervised_learning.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>What features might you want to use for text classification? </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Data:\n",
    "    - Individual words\n",
    "    - Phrases\n",
    "    - Punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Metadata:\n",
    "    - Length of document\n",
    "    - Language\n",
    "    - Sender or receiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<center><h2>Naive Bayes \"embarrassingly parallelizable\"</h2></center>\n",
    "\n",
    "Each feature weight can be learned separately for each class.\n",
    "\n",
    "Thus it can be learned at the same time on a separate computer / core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Source: https://alitarhini.wordpress.com/2011/03/02/parallel-naive-bayesian-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
